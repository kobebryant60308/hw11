{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1869f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef862041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f9561f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d76b2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a317eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07312187",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6f5978a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "920620c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2838aa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88a5f998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 80.2M    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 80.2M    0  176k    0     0  99325      0  0:14:06  0:00:01  0:14:05 99351\n",
      "  2 80.2M    2 2128k    0     0   749k      0  0:01:49  0:00:02  0:01:47  749k\n",
      "  6 80.2M    6 5280k    0     0  1367k      0  0:01:00  0:00:03  0:00:57 1367k\n",
      " 10 80.2M   10 8832k    0     0  1807k      0  0:00:45  0:00:04  0:00:41 1808k\n",
      " 15 80.2M   15 12.3M    0     0  2177k      0  0:00:37  0:00:05  0:00:32 2609k\n",
      " 19 80.2M   19 16.0M    0     0  2412k      0  0:00:34  0:00:06  0:00:28 3254k\n",
      " 24 80.2M   24 19.7M    0     0  2597k      0  0:00:31  0:00:07  0:00:24 3654k\n",
      " 29 80.2M   29 23.7M    0     0  2756k      0  0:00:29  0:00:08  0:00:21 3839k\n",
      " 34 80.2M   34 27.7M    0     0  2893k      0  0:00:28  0:00:09  0:00:19 3964k\n",
      " 39 80.2M   39 31.8M    0     0  3004k      0  0:00:27  0:00:10  0:00:17 3954k\n",
      " 44 80.2M   44 35.9M    0     0  3097k      0  0:00:26  0:00:11  0:00:15 4015k\n",
      " 49 80.2M   49 39.9M    0     0  3192k      0  0:00:25  0:00:12  0:00:13 4121k\n",
      " 54 80.2M   54 43.9M    0     0  3258k      0  0:00:25  0:00:13  0:00:12 4142k\n",
      " 59 80.2M   59 48.0M    0     0  3323k      0  0:00:24  0:00:14  0:00:10 4172k\n",
      " 65 80.2M   65 52.4M    0     0  3399k      0  0:00:24  0:00:15  0:00:09 4265k\n",
      " 71 80.2M   71 57.4M    0     0  3494k      0  0:00:23  0:00:16  0:00:07 4447k\n",
      " 78 80.2M   78 62.8M    0     0  3611k      0  0:00:22  0:00:17  0:00:05 4681k\n",
      " 85 80.2M   85 68.8M    0     0  3748k      0  0:00:21  0:00:18  0:00:03 5100k\n",
      " 93 80.2M   93 75.3M    0     0  3897k      0  0:00:21  0:00:19  0:00:02 5596k\n",
      "100 80.2M  100 80.2M    0     0  4013k      0  0:00:20  0:00:20 --:--:-- 6096k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb29408b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' 不是內部或外部命令、可執行的程式或批次檔。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41059b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' 不是內部或外部命令、可執行的程式或批次檔。\n"
     ]
    }
   ],
   "source": [
    "!cat aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89dd92b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "906f3d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70000 files belonging to 3 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64814316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'Someone keeps asking me, \"What\\'s this movie about?\" SS5+ is one of those that defies to be labeled...just the way I like it.<br /><br />Tadanobu Asano, a literal jack of all trades, is the main reason I wanted to see this. From an infamous masochist king known as Kakihara to low-key Kenji in the visually stunning Last Life in the Universe, Asano always manages to display, not only a wide range, but he performs on several different levels. In this one, you can\\'t help feel anticipation for his scenes and trust me: they\\'re worth the wait!!!You are then presented to an elegant buffet of avid brights, darks and color schemes which makes his home seem like Lewis Carroll\\'s summer house. Reika Hashimoto and Asano (phenom chem!) had me mind scrapingly perplexed as to what would unfold next.<br /><br />The rest of the cast all play their roles well (and with fun) and the intertwining events keep the ball rolling with an occasional rest stop for character development and goodfeel. There\\'s a dark humor in this film which gives it it an odd, adderol driven, yet refreshingly original style. Also, a high bpm soundtrack gives it some additional high octane for the trip. I will have to keep an eye out for Gen Sekiguchi\\'s future films or other works by Taku Tada \\'cause Survive Style 5+ has warped my way of thinking \\xc2\\x96 in a good way!', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(2, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccf42fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b18eec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(2, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49490afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38ed8f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "2188/2188 [==============================] - 30s 13ms/step - loss: -750.5076 - accuracy: 0.1429 - val_loss: 1840.8898 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "2188/2188 [==============================] - 10s 5ms/step - loss: -4564.2241 - accuracy: 0.1429 - val_loss: 6736.1406 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "2188/2188 [==============================] - 10s 5ms/step - loss: -11797.7412 - accuracy: 0.1429 - val_loss: 14680.4082 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "2188/2188 [==============================] - 10s 5ms/step - loss: -22505.4473 - accuracy: 0.1429 - val_loss: 25677.1172 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "2188/2188 [==============================] - 10s 5ms/step - loss: -36810.8828 - accuracy: 0.1429 - val_loss: 39714.5742 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "2188/2188 [==============================] - 11s 5ms/step - loss: -54499.6055 - accuracy: 0.1429 - val_loss: 56811.0977 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "2188/2188 [==============================] - 10s 5ms/step - loss: -75472.5469 - accuracy: 0.1429 - val_loss: 76947.0469 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "2188/2188 [==============================] - 10s 5ms/step - loss: -99896.1406 - accuracy: 0.1429 - val_loss: 100133.0625 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "2188/2188 [==============================] - 10s 5ms/step - loss: -127971.6172 - accuracy: 0.1429 - val_loss: 126391.1562 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "2188/2188 [==============================] - 10s 5ms/step - loss: -159127.2969 - accuracy: 0.1429 - val_loss: 155651.8750 - val_accuracy: 0.5000\n",
      "782/782 [==============================] - 18s 22ms/step - loss: 1830.3007 - accuracy: 0.5000\n",
      "Test acc: 0.500\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1baa10f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3c025e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "2188/2188 [==============================] - 24s 11ms/step - loss: -1035.1091 - accuracy: 0.1429 - val_loss: 2553.9910 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "2188/2188 [==============================] - 11s 5ms/step - loss: -6378.6714 - accuracy: 0.1429 - val_loss: 9434.3301 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "2188/2188 [==============================] - 11s 5ms/step - loss: -16645.7051 - accuracy: 0.1429 - val_loss: 20637.6328 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "2188/2188 [==============================] - 11s 5ms/step - loss: -31843.5664 - accuracy: 0.1429 - val_loss: 36155.6562 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "2188/2188 [==============================] - 11s 5ms/step - loss: -51811.9258 - accuracy: 0.1429 - val_loss: 55968.9023 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "2188/2188 [==============================] - 11s 5ms/step - loss: -76664.1406 - accuracy: 0.1429 - val_loss: 80074.3125 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "2188/2188 [==============================] - 11s 5ms/step - loss: -106732.4844 - accuracy: 0.1429 - val_loss: 108555.7500 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "2188/2188 [==============================] - 12s 5ms/step - loss: -141006.5625 - accuracy: 0.1429 - val_loss: 141271.4531 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "2188/2188 [==============================] - 11s 5ms/step - loss: -180687.9844 - accuracy: 0.1429 - val_loss: 178327.3594 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "2188/2188 [==============================] - 11s 5ms/step - loss: -224788.8125 - accuracy: 0.1429 - val_loss: 219649.8438 - val_accuracy: 0.5000\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 2537.5701 - accuracy: 0.5000\n",
      "Test acc: 0.500\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2ab35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44ed3ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "631a0beb",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa8 in position 121: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtext_vectorization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_only_train_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m tfidf_2gram_train_ds \u001b[38;5;241m=\u001b[39m train_ds\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x, y: (text_vectorization(x), y),\n\u001b[0;32m      5\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      6\u001b[0m tfidf_2gram_val_ds \u001b[38;5;241m=\u001b[39m val_ds\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x, y: (text_vectorization(x), y),\n\u001b[0;32m      8\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\preprocessing\\text_vectorization.py:467\u001b[0m, in \u001b[0;36mTextVectorization.adapt\u001b[1;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madapt\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;124;03m\"\"\"Computes a vocabulary of string terms from tokens in a dataset.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \n\u001b[0;32m    420\u001b[0m \u001b[38;5;124;03m    Calling `adapt()` on a `TextVectorization` layer is an alternative to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m          argument is not supported with array inputs.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py:258\u001b[0m, in \u001b[0;36mPreprocessingLayer.adapt\u001b[1;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m--> 258\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_adapt_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m    260\u001b[0m             context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa8 in position 121: invalid start byte"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5a391ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exception encountered when calling layer \"string_lookup_5\" \"                 f\"(type StringLookup).\n\nWhen using `output_mode=tf_idf` and `pad_to_max_tokens=False`, you must set the layer's vocabulary before calling it. Either pass a `vocabulary` argument to the layer, or call `adapt` with some sample data.\n\nCall arguments received by layer \"string_lookup_5\" \"                 f\"(type StringLookup):\n  • inputs=tf.RaggedTensor(values=Tensor(\"text_vectorization_5/StringNGrams/StringNGrams:0\", shape=(None,), dtype=string), row_splits=Tensor(\"text_vectorization_5/StringNGrams/StringNGrams:1\", shape=(None,), dtype=int64))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtext_vectorization\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(processed_inputs)\n\u001b[0;32m      4\u001b[0m inference_model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mModel(inputs, outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\preprocessing\\index_lookup.py:885\u001b[0m, in \u001b[0;36mIndexLookup._maybe_freeze_vocab_size\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    883\u001b[0m     new_vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_size()\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_vocab_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_start_index():\n\u001b[1;32m--> 885\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    886\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen using `output_mode=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` and `pad_to_max_tokens=False`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    887\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou must set the layer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms vocabulary before calling it. Either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a `vocabulary` argument to the layer, or call `adapt` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    889\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith some sample data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_mode)\n\u001b[0;32m    890\u001b[0m     )\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frozen_vocab_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m new_vocab_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frozen_vocab_size\n\u001b[0;32m    894\u001b[0m ):\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    896\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen using `output_mode=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` and `pad_to_max_tokens=False`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    897\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe vocabulary size cannot be changed after the layer is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    900\u001b[0m         )\n\u001b[0;32m    901\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Exception encountered when calling layer \"string_lookup_5\" \"                 f\"(type StringLookup).\n\nWhen using `output_mode=tf_idf` and `pad_to_max_tokens=False`, you must set the layer's vocabulary before calling it. Either pass a `vocabulary` argument to the layer, or call `adapt` with some sample data.\n\nCall arguments received by layer \"string_lookup_5\" \"                 f\"(type StringLookup):\n  • inputs=tf.RaggedTensor(values=Tensor(\"text_vectorization_5/StringNGrams/StringNGrams:0\", shape=(None,), dtype=string), row_splits=Tensor(\"text_vectorization_5/StringNGrams/StringNGrams:1\", shape=(None,), dtype=int64))"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "outputs = model(processed_inputs)\n",
    "inference_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa3a655f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inference_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [26], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m raw_text_data \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor([\n\u001b[0;32m      3\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThat was an excellent movie, I loved it.\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      4\u001b[0m ])\n\u001b[1;32m----> 5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43minference_model\u001b[49m(raw_text_data)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mfloat\u001b[39m(predictions[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m percent positive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inference_model' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "raw_text_data = tf.convert_to_tensor([\n",
    "    [\"That was an excellent movie, I loved it.\"],\n",
    "])\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4d47b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
